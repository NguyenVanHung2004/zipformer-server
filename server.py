import asyncio
import websockets
import sherpa_onnx
import os
import json
import numpy as np
import logging
import sys
from collections import deque
import glob
import urllib.request
import tarfile
import shutil

# --- AUTO-DOWNLOAD MODELS (For No-Dockerfile Deployment) ---
def check_and_download_models():
    # 1. Define URLs
    asr_url = "https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-zipformer-vi-2025-04-20.tar.bz2"
    vad_url = "https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/silero_vad.onnx"
    
    # 2. Check & Download VAD
    if not os.path.exists("model_vi/silero_vad.onnx"):
        print("‚è≥ Downloading VAD model...")
        os.makedirs("model_vi", exist_ok=True)
        try:
            urllib.request.urlretrieve(vad_url, "model_vi/silero_vad.onnx")
            print("‚úÖ VAD Downloaded")
        except Exception as e:
            print(f"‚ùå VAD Download Failed: {e}")

    # 3. Check & Download ASR
    # Check if ANY encoder file exists
    if not glob.glob("model_vi/encoder-*.onnx"):
        print(f"‚è≥ Downloading ASR Model from {asr_url}...")
        try:
            filename = "asr_model.tar.bz2"
            urllib.request.urlretrieve(asr_url, filename)
            print("üì¶ Extracting ASR...")
            with tarfile.open(filename, "r:bz2") as tar:
                tar.extractall(".")
            
            # Move files from 'sherpa-onnx-zipformer-vi-2025-04-20' to 'model_vi'
            extracted_dir = "sherpa-onnx-zipformer-vi-2025-04-20"
            if os.path.exists(extracted_dir):
                os.makedirs("model_vi", exist_ok=True) 
                
                # Delete existing .onnx files in model_vi to avoid conflicts
                for f in glob.glob("model_vi/*.onnx"):
                    os.remove(f)
                
                for f in os.listdir(extracted_dir):
                    shutil.move(os.path.join(extracted_dir, f), "model_vi")
                os.rmdir(extracted_dir)
            
            if os.path.exists(filename): os.remove(filename)
            print("‚úÖ ASR Model Ready")
        except Exception as e:
            print(f"‚ùå ASR Download Failed: {e}")

# Run check immediately
check_and_download_models()



# --- CONFIGURATION ---
PORT = int(os.environ.get("PORT", 6006))
# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def create_components():
    model_dir = "./model_vi"
    
    # Dynamic file finding
    tokens_path = os.path.join(model_dir, "tokens.txt")
    enc_files = glob.glob(os.path.join(model_dir, "encoder-*.onnx"))
    dec_files = glob.glob(os.path.join(model_dir, "decoder-*.onnx"))
    join_files = glob.glob(os.path.join(model_dir, "joiner-*.onnx"))
    
    if not (enc_files and dec_files and join_files):
         logging.error(f"‚ùå Could not find model files in {model_dir}")
         sys.exit(1)
         
    encoder_path = enc_files[0]
    decoder_path = dec_files[0]
    joiner_path = join_files[0]
    
    vad_model = os.path.join(model_dir, "silero_vad.onnx")

    if not all(os.path.exists(p) for p in [tokens_path, encoder_path, decoder_path, joiner_path, vad_model]):
        logging.error(f"‚ùå Thi·∫øu model files! Ki·ªÉm tra th∆∞ m·ª•c '{model_dir}'.")
        sys.exit(1)

    logging.info("‚è≥ ƒêang t·∫£i Offline Recognizer...")
    recognizer = sherpa_onnx.OfflineRecognizer.from_transducer(
        tokens=tokens_path,
        encoder=encoder_path,
        decoder=decoder_path,
        joiner=joiner_path,
        num_threads=4,
        sample_rate=16000,
        feature_dim=80,
        decoding_method="greedy_search",
    )
    
    logging.info("‚è≥ ƒêang t·∫£i VAD...")
    vad_config = sherpa_onnx.VadModelConfig()
    vad_config.silero_vad.model = vad_model
    vad_config.sample_rate = 16000
    vad = sherpa_onnx.VoiceActivityDetector(vad_config, buffer_size_in_seconds=60)

    logging.info("‚úÖ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng!")
    return recognizer, vad

recognizer, vad = create_components()

async def handle_connection(websocket):
    logging.info("üîó Client ƒë√£ k·∫øt n·ªëi")
    
    # M·ªói client c·∫ßn m·ªôt instance VAD ri√™ng bi·ªát n·∫øu mu·ªën stateful ch√≠nh x√°c, 
    # nh∆∞ng sherpa_onnx.VoiceActivityDetector c√≥ v·∫ª gi·ªØ state buffer. 
    # Tuy nhi√™n, doc m·∫´u ch·ªâ d√πng 1 global vad n·∫øu ƒë∆°n lu·ªìng. 
    # ƒêa lu·ªìng: T·ªët nh·∫•t n√™n t·∫°o VAD m·ªõi cho m·ªói conn ho·∫∑c ƒë·∫£m b·∫£o thread-safe.
    # ƒê·ªÉ an to√†n v√† ƒë∆°n gi·∫£n, ta s·∫Ω t·∫°o l·∫°i VAD cho m·ªói connection ho·∫∑c reset.
    # Nh∆∞ng VAD load model c≈©ng nh·∫π. Ta s·∫Ω init l·∫°i config clone t·ª´ global ho·∫∑c l√†m m·ªõi.
    
    # RE-INIT VAD for each client to avoid buffer mixing
    model_dir = "./model_vi"
    vad_model = os.path.join(model_dir, "silero_vad.onnx")
    vad_config = sherpa_onnx.VadModelConfig()
    vad_config.silero_vad.model = vad_model
    vad_config.sample_rate = 16000
    
    # ‚ö° TUNING VAD PARAMETERS (NOISE REDUCTION)
    # TƒÉng threshold l√™n 0.6 ƒë·ªÉ l·ªçc ti·∫øng chu·ªôt/ph√≠m (ch·ªâ gi·ªçng n√≥i r√µ m·ªõi b·∫Øt)
    vad_config.silero_vad.threshold = 0.6         
    vad_config.silero_vad.min_silence_duration = 0.5 
    # TƒÉng min_speech l√™n 0.5s ƒë·ªÉ b·ªè qua ti·∫øng click ng·∫Øn
    vad_config.silero_vad.min_speech_duration = 0.5 
    
    client_vad = sherpa_onnx.VoiceActivityDetector(vad_config, buffer_size_in_seconds=60)
    
    # üéß BUFFERING LOGIC (ƒê·ªÉ b·∫Øt l·∫°i ƒëo·∫°n ƒë·∫ßu b·ªã m·∫•t)
    # L∆∞u gi·ªØ 0.5 gi√¢y √¢m thanh tr∆∞·ªõc ƒë√≥ (16000 * 0.5 = 8000 m·∫´u)
    from collections import deque
    # M·ªói chunk t·ª´ client l√† 1 l∆∞·ª£ng samples nh·∫•t ƒë·ªãnh, ta l∆∞u raw samples v√†o deque
    # Tuy nhi√™n deque l∆∞u t·ª´ng item, n·∫øu item l√† chunk to th√¨ kh√≥ qu·∫£n l√Ω size ch√≠nh x√°c.
    # Ta s·∫Ω l∆∞u list c√°c array, v√† estimte size.
    # ƒê∆°n gi·∫£n h∆°n: l∆∞u 1 buffer v√≤ng tr√≤n b·∫±ng numpy array nh∆∞ng t·ªën chi ph√≠ copy.
    # C√°ch hi·ªáu qu·∫£: Deque ch·ª©a c√°c chunk, t·ªïng duration ~0.5s.
    
    pre_speech_buffer = deque(maxlen=20) # Gi·∫£ s·ª≠ m·ªói chunk ~50ms -> 20 chunks = 1s
    
    try:
        async for message in websocket:
            # message l√† bytes (audio chunk)
            samples = np.frombuffer(message, dtype=np.int16).astype(np.float32) / 32768.0
            
            # Th√™m v√†o buffer l·ªãch s·ª≠
            pre_speech_buffer.append(samples)
            
            client_vad.accept_waveform(samples)
            
            while not client_vad.empty():
                speech_segment = client_vad.front.samples
                # [NEW] L·∫•y offset c·ªßa segment n√†y trong c·∫£ chu·ªói streaming
                # client_vad.front.start l√† index sample b·∫Øt ƒë·∫ßu c·ªßa segment
                segment_offset_seconds = 0.0
                if hasattr(client_vad.front, 'start'):
                     segment_offset_seconds = client_vad.front.start / 16000.0
                
                client_vad.pop()
                
                if len(speech_segment) < 1000: # B·ªè qua ƒëo·∫°n qu√° ng·∫Øn (< 0.06s)
                    continue
                
                # ... (Padding code omitted for brevity, ensure we adjust offset if padding is used? 
                # Actually padding is prepended *before* this segment in my logic, 
                # but physically concatenating it changes the relative time in `recognizer`.
                # If I prepend history, the recognizer sees [HISTORY + SEGMENT].
                # The Recognizer timestamps start at 0.
                # So relative to stream:
                # Real start = segment_offset_seconds - duration(history)
                # Let's handle this carefully.
                
                prepend_duration = 0.0
                if pre_speech_buffer:
                    history_samples = np.concatenate(list(pre_speech_buffer))
                    if len(history_samples) > 8000:
                        history_samples = history_samples[-8000:]
                    
                    prepend_duration = len(history_samples) / 16000.0
                    speech_segment = np.concatenate((history_samples, speech_segment))
                
                logging.info(f"üó£Ô∏è Ph√°t hi·ªán ti·∫øng n√≥i ({len(speech_segment)/16000:.2f}s). Offset: {segment_offset_seconds:.2f}s")
                
                stream = recognizer.create_stream()
                stream.accept_waveform(16000, speech_segment)
                recognizer.decode_stream(stream)
                result = stream.result
                
                # [FIX]: Reconstruct text from tokens
                if hasattr(result, 'tokens'):
                    raw_tokens = result.tokens
                    reconstructed_text = "".join(raw_tokens).replace('‚ñÅ', ' ').strip()
                    import re
                    text = re.sub(r'\s+', ' ', reconstructed_text)
                else:
                    text = result.text.strip()

                if text:
                    words = []
                    if hasattr(result, 'tokens') and hasattr(result, 'timestamps'):
                        for i, token in enumerate(result.tokens):
                            # Timestamp t·ª´ recognizer l√† relative so v·ªõi ƒë·∫ßu speech_segment (ƒë√£ g·ªìm padding)
                            local_start = result.timestamps[i]
                            
                            # Chuy·ªÉn sang Absolute Timestamp
                            # Absolute = Segment_Start_In_Stream - Prepend_Duration + Local_Start
                            absolute_start = segment_offset_seconds - prepend_duration + local_start
                            
                            # ƒê·∫£m b·∫£o kh√¥ng √¢m
                            absolute_start = max(0.0, absolute_start)
                            
                            start = absolute_start
                            end = start + 0.1
                            if i < len(result.timestamps) - 1:
                                next_local = result.timestamps[i+1]
                                next_absolute = segment_offset_seconds - prepend_duration + next_local
                                end = next_absolute
                            
                            clean_word = token.replace('‚ñÅ', '').strip()
                            words.append({
                                "word": clean_word,
                                "start": start,
                                "end": end,
                                "confidence": 1.0,
                                "speaker": 0
                            })

                    # Deepgram-compatible format
                    response = {
                        "channel": {
                            "alternatives": [
                                {
                                    "transcript": text,
                                    "confidence": 1.0,
                                    "words": words
                                }
                            ]
                        },
                        "is_final": True
                    }
                    logging.info(f"üìù K·∫øt qu·∫£: {text}")
                    await websocket.send(json.dumps(response, ensure_ascii=False))

    except websockets.exceptions.ConnectionClosed:
        logging.info("üîå Client ƒë√£ ng·∫Øt k·∫øt n·ªëi")
    except Exception as e:
        logging.error(f"‚ùå L·ªói connection: {e}")

async def main():
    server = await websockets.serve(handle_connection, "0.0.0.0", PORT)
    logging.info(f"üöÄ Server (VAD + Offline) ƒëang l·∫Øng nghe t·∫°i ws://0.0.0.0:{PORT}")
    await server.wait_closed()

if __name__ == "__main__":
    asyncio.run(main())